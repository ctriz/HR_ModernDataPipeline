{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v6.json", "dbt_version": "1.10.10", "generated_at": "2025-09-03T16:41:12.695175Z", "invocation_id": "83242e35-1b94-44c0-9199-af16494e2611", "invocation_started_at": "2025-09-03T16:40:30.527336Z", "env": {}}, "results": [{"status": "error", "timing": [{"name": "compile", "started_at": "2025-09-03T16:40:52.377832Z", "completed_at": "2025-09-03T16:40:52.393491Z"}, {"name": "execute", "started_at": "2025-09-03T16:40:52.393491Z", "completed_at": "2025-09-03T16:41:12.628402Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 20.281043529510498, "adapter_response": {}, "message": "Runtime Error in model gold_attrition_prototype (models\\gold\\gold_attrition_prototype.sql)\n  An error occurred while calling o31.sql.\n  : java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\r\n  \tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n  \tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n  \tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n  \tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n  \tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getGroup(RawLocalFileSystem.java:832)\r\n  \tat org.apache.hadoop.hive.io.HdfsUtils.setFullFileStatus(HdfsUtils.java:102)\r\n  \tat org.apache.hadoop.hive.io.HdfsUtils.setFullFileStatus(HdfsUtils.java:94)\r\n  \tat org.apache.hadoop.hive.io.HdfsUtils.setFullFileStatus(HdfsUtils.java:77)\r\n  \tat org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:544)\r\n  \tat org.apache.hadoop.hive.metastore.Warehouse.mkdirs(Warehouse.java:194)\r\n  \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1437)\r\n  \tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)\r\n  \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n  \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n  \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  \tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n  \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\r\n  \tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\r\n  \tat jdk.proxy2/jdk.proxy2.$Proxy38.create_table_with_environment_context(Unknown Source)\r\n  \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\r\n  \tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\r\n  \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\r\n  \tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\r\n  \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n  \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n  \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  \tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n  \tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\r\n  \tat jdk.proxy2/jdk.proxy2.$Proxy39.createTable(Unknown Source)\r\n  \tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\r\n  \tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\r\n  \tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\r\n  \tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:573)\r\n  \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n  \tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)\r\n  \tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)\r\n  \tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\r\n  \tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\r\n  \tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\r\n  \tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:520)\r\n  \tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:426)\r\n  \tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:273)\r\n  \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n  \tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101)\r\n  \tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:244)\r\n  \tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\r\n  \tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:402)\r\n  \tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:423)\r\n  \tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:238)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\r\n  \tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:51)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\r\n  \tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\r\n  \tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\r\n  \tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:51)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\r\n  \tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:51)\r\n  \tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:109)\r\n  \tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:167)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\r\n  \tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)\r\n  \tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:87)\r\n  \tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:489)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\r\n  \tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\r\n  \tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)\r\n  \tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:451)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:603)\r\n  \tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:587)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:582)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:202)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:236)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n  \tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n  \tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n  \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n  \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n  \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n  \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n  \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n  \tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n  \tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n  \tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n  \tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n  \tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n  \tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n  \tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n  \tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n  \tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n  \tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n  \tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n  \tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n  \tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\r\n  \tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n  \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n  \tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n  \tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\r\n  \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n  \tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\r\n  \tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\r\n  \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n  \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n  \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  \tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n  \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n  \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n  \tat py4j.Gateway.invoke(Gateway.java:282)\r\n  \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n  \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n  \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n  \tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n  \tat java.base/java.lang.Thread.run(Thread.java:842)\r\n  ", "failures": null, "unique_id": "model.hrdatapipeline.gold_attrition_prototype", "compiled": true, "compiled_code": "\n\nWITH base AS (\n    SELECT\n        employee_id,\n        department_id,\n        job_id,\n        location_id,\n        updated_at,\n        is_deleted,\n        YEAR(updated_at) AS year,\n        MONTH(updated_at) AS month\n    FROM delta.`file:///C:/UpScale/HRProject/HRDataPipeline/analytics/delta/bronze/employees`\n)\n\nSELECT\n    year,\n    month,\n    COUNT(CASE WHEN is_deleted = true THEN 1 END) AS attrition_count,\n    COUNT(*) AS total_employees,\n    COALESCE(\n    COUNT(CASE WHEN is_deleted = true THEN 1 END) / NULLIF(COUNT(*), 0),\n    0\n    ) AS attrition_rate\nFROM base\nGROUP BY year, month\nORDER BY year, month", "relation_name": "default_gold.gold_attrition_prototype", "batch_results": null}], "elapsed_time": 40.647778272628784, "args": {"macro_debugging": false, "project_dir": "C:\\UpScale\\HRProject\\HRDataPipeline\\dbt", "log_format_file": "debug", "require_yaml_configuration_for_mf_time_spines": false, "populate_cache": true, "skip_nodes_if_on_run_start_fails": false, "require_resource_names_without_spaces": true, "favor_state": false, "require_all_warnings_handled_by_warn_error": false, "select": [], "state_modified_compare_more_unrendered_values": false, "require_nested_cumulative_type_params": false, "invocation_command": "dbt run", "defer": false, "exclude": [], "quiet": false, "upload_to_artifacts_ingest_api": false, "write_json": true, "print": true, "which": "run", "introspect": true, "log_format": "default", "source_freshness_run_project_hooks": true, "require_batched_execution_for_custom_microbatch_strategy": false, "static_parser": true, "vars": {}, "require_explicit_package_overrides_for_builtin_materializations": true, "empty": false, "use_fast_test_edges": false, "show_all_deprecations": false, "state_modified_compare_vars": false, "send_anonymous_usage_stats": true, "require_generic_test_arguments_property": true, "indirect_selection": "eager", "log_level": "info", "partial_parse_file_diff": true, "use_colors": true, "cache_selected_only": false, "validate_macro_args": false, "partial_parse": true, "log_path": "C:\\UpScale\\HRProject\\HRDataPipeline\\dbt\\logs", "version_check": true, "printer_width": 80, "show_resource_report": false, "profiles_dir": "C:\\UpScale\\HRProject\\HRDataPipeline\\dbt", "log_level_file": "debug", "strict_mode": false, "warn_error_options": {"error": [], "warn": [], "silence": []}, "use_colors_file": true, "log_file_max_bytes": 10485760}}